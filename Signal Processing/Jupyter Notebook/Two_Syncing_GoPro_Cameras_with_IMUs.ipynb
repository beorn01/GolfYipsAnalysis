{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for condition \n",
    "global participant\n",
    "participant = '050'\n",
    "\n",
    "# Global variables for condition \n",
    "global condition\n",
    "condition = 'Ball'  #'Ball' or 'NB'\n",
    "\n",
    "#and hand type\n",
    "global hand_type\n",
    "hand_type = 'LH' #BH RH LH\n",
    "\n",
    "\n",
    "# Configuration cell for video paths\n",
    "global video_paths\n",
    "video_paths = {\n",
    "    #\"Videopanel_Sync\": f\"/Users/beorn/Dropbox/0WORK/3Project Wensen/EMG Kin Research Experiment/Data/Experiment Data/SDC_{participant}/Videos/Videopanel/SDC_{participant}_{condition}_{hand_type}_VP.MP4\"\n",
    "    # \"Videopanel_Sync\": f\"/Volumes/Samsung_T5/Experiment Data/SDC_{participant}/Videos/Videopanel/SDC_{participant}_{condition}_{hand_type}_VP.MP4\"\n",
    "    \"Videopanel_Sync\": f\"/Volumes/Beorn_4T/Experiment Data/SDC_{participant}/Videos/Videopanel/SDC_{participant}_{condition}_{hand_type}_VP.MP4\"\n",
    "    # \"Videopanel_Sync\": f\"D:\\Experiment Data\\SDC_{participant}\\Videos\\Videopanel\\SDC_{participant}_{condition}_{hand_type}_VP.MP4\"\n",
    "\n",
    "}\n",
    "\n",
    "# Configuration cell for video paths\n",
    "global file_paths\n",
    "file_paths = {\n",
    "    # \"File_Sync\": f\"/Users/beorn/Dropbox/0WORK/3Project Wensen/EMG Kin Research Experiment/Data/Experiment Data/SDC_{participant}/EMG/\"\n",
    "    \"File_Sync\": f\"/Volumes/Beorn_4T/Experiment Data/SDC_{participant}/EMG/\"\n",
    "    # \"File_Sync\": f\"D:\\Experiment Data\\SDC_{participant}\\EMG\"\n",
    "}\n",
    "\n",
    "# Configuration cell for video paths\n",
    "global write_paths\n",
    "write_paths = {\n",
    "    #\"Write_Sync\": f\"/Users/beorn/Dropbox/0WORK/3Project Wensen/EMG Kin Research Experiment/Data/Experiment Data/SDC_{participant}/Videos/Videopanel/TESTING_Frame_Sync.MP4\"\n",
    "    \"Write_Sync\": f\"/Volumes/Beorn_4T/Experiment Data/SDC_{participant}/Videos/Videopanel/TESTING_Frame_Sync.MP4\"\n",
    "    # \"Write_Sync\": f\"D:\\Experiment Data\\SDC_{participant}\\Videos/Videopanel\\TESTING_Frame_Sync.MP4\"\n",
    "}\n",
    "\n",
    "# Configuration cell for video paths\n",
    "global save_paths\n",
    "save_paths = {\n",
    "    # \"Save_Sync\": f\"/Users/beorn/Dropbox/0WORK/3Project Wensen/EMG Kin Research Experiment/Data/Experiment Data/SDC_{participant}/aligned_data.mat\"\n",
    "    \"Save_Sync\": f\"/Volumes/Beorn_4T/Experiment Data/SDC_{participant}/aligned_data.mat\"\n",
    "    # \"Save_Sync\": f\"D:\\Experiment Data\\SDC_{participant}\\aligned_data.mat\"    \n",
    "    \n",
    "}\n",
    "\n",
    "# Configuration cell for video paths\n",
    "global aligned_dat_save\n",
    "aligned_data_save = {\n",
    "    # \"aligned_data_path\": f\"/Users/beorn/Dropbox/0WORK/3Project Wensen/EMG Kin Research Experiment/Data/Experiment Data/SDC_{participant}/Time_Axis_Alignmen_Python.json\"\n",
    "    \"aligned_data_path\": f\"/Volumes/Beorn_4T/Experiment Data/SDC_{participant}/Time_Axis_Alignmen_Python.json\"\n",
    "    # \"aligned_data_path\": f\"D:\\Experiment Data\\SDC_{participant}\\Time_Axis_Alignmen_Python.json\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating start time for GoPro Video and producin ga time vector based on that\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define the paths to your video files using the video_paths dictionary\n",
    "video_file_path = video_paths[\"Videopanel_Sync\"]  # Ensure the key matches exactly what you defined above\n",
    "\n",
    "\n",
    "# # Define the path to your video file\n",
    "# video_file_path = \"/Users/beorn/Dropbox/0WORK/3Project Wensen/EMG Kin Research Experiment/Data/Experiment Data/SDC_007/Videos/Videopanel/SDC_007_Ball_RH_VP_synced_time_injection.MP4\"\n",
    "\n",
    "\n",
    "# Construct the ffprobe command to extract specific metadata\n",
    "metadata_command = (\n",
    "    f'ffprobe -v error '\n",
    "    f'-show_entries stream_tags=timecode '\n",
    "    f'-show_entries format_tags=creation_time '\n",
    "    f'-select_streams v:0 '  # Focus on video stream for timecode; adjust if needed\n",
    "    f'-show_entries stream=nb_frames,r_frame_rate,duration '\n",
    "    f'-of default=noprint_wrappers=1 \"{video_file_path}\"'\n",
    ")\n",
    "\n",
    "# Execute the command and capture the output\n",
    "metadata_process = subprocess.run(metadata_command, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Check if the command was successful\n",
    "if metadata_process.returncode == 0:\n",
    "    print(\"Metadata extracted successfully.\")\n",
    "    \n",
    "    # Initialize a dictionary to hold the metadata\n",
    "    data = {'Total Frames': None, 'Timecode': None, 'Creation Time': None, 'Frame Rate': None}\n",
    "    \n",
    "    # Parse the output\n",
    "    for line in metadata_process.stdout.split('\\n'):\n",
    "        if 'nb_frames' in line:\n",
    "            key, value = line.split('=')\n",
    "            data['Total Frames'] = value.strip() if value.strip().isdigit() else 'Calculation needed'\n",
    "        elif 'TAG:creation_time' in line:\n",
    "            _, value = line.split('=')\n",
    "            data['Creation Time'] = value.strip()\n",
    "        elif 'TAG:timecode' in line:\n",
    "            _, value = line.split('=')\n",
    "            data['Timecode'] = value.strip()\n",
    "        elif 'r_frame_rate' in line:\n",
    "            _, value = line.split('=')\n",
    "            # Simplify frame rate to a single number if possible\n",
    "            numerator, denominator = map(int, value.strip().split('/'))\n",
    "            data['Frame Rate'] = str(numerator / denominator if denominator != 0 else numerator)\n",
    "    \n",
    "    # Convert the dictionary into a DataFrame for nicer display\n",
    "    df = pd.DataFrame(list(data.items()), columns=['Tag', 'Value'])\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Error extracting metadata:\", metadata_process.stderr)\n",
    "\n",
    "\n",
    "    \n",
    "# Assuming 'df' is your DataFrame\n",
    "creation_time_str = df.loc[df['Tag'] == 'Creation Time', 'Value'].iloc[0]\n",
    "timecode_str = df.loc[df['Tag'] == 'Timecode', 'Value'].iloc[0]\n",
    "frame_rate = df.loc[df['Tag'] == 'Frame Rate', 'Value'].iloc[0]  # Directly using frame rate value\n",
    "\n",
    "# Splitting creation time to get the date and hour part\n",
    "creation_date_hour = creation_time_str.split('T')[0]\n",
    "creation_hour = creation_time_str.split('T')[1][:2]\n",
    "\n",
    "# Extracting minutes, seconds, and hundredths from the timecode\n",
    "timecode_minutes = timecode_str.split(':')[1]\n",
    "timecode_seconds = timecode_str.split(':')[2]\n",
    "timecode_hundredths = timecode_str.split(':')[3]\n",
    "\n",
    "# Combine them without altering the frame rate usage\n",
    "combined_timecode = f\"{creation_date_hour}T{creation_hour}:{timecode_minutes}:{timecode_seconds}.{timecode_hundredths}Z\"\n",
    "\n",
    "print(\"Combined Timecode:\", combined_timecode)\n",
    "print(\"Frame Rate:\", frame_rate)\n",
    "\n",
    "\n",
    "# Assuming combined_timecode is your final timecode from previous steps\n",
    "combined_timecode_str = f\"{creation_date_hour}T{creation_hour}:{timecode_minutes}:{timecode_seconds}.{timecode_hundredths}Z\"\n",
    "\n",
    "# Parse the combined timecode into a datetime object\n",
    "combined_timecode_datetime = datetime.strptime(combined_timecode_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "# Convert the datetime object to UNIX timestamp\n",
    "unix_timestamp = combined_timecode_datetime.replace(tzinfo=timezone.utc).timestamp()\n",
    "\n",
    "print(\"UNIX Timestamp:\", unix_timestamp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming combined_timecode_datetime is already defined\n",
    "unix_timestamp_microseconds = int(combined_timecode_datetime.replace(tzinfo=timezone.utc).timestamp() * 1e6)\n",
    "\n",
    "print(\"UNIX Timestamp (microseconds):\", unix_timestamp_microseconds)\n",
    "\n",
    "\n",
    "# Assuming df is your existing DataFrame and unix_timestamp_microseconds is defined\n",
    "new_row = pd.DataFrame({'Tag': ['UNIX Time'], 'Value': [str(unix_timestamp_microseconds)]})\n",
    "\n",
    "# Using pandas.concat to add the new row\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "# Extract values from DataFrame\n",
    "unix_time_start = int(df.loc[df['Tag'] == 'UNIX Time', 'Value'].iloc[0])\n",
    "frame_rate = float(df.loc[df['Tag'] == 'Frame Rate', 'Value'].iloc[0])\n",
    "total_frames = int(df.loc[df['Tag'] == 'Total Frames', 'Value'].iloc[0])\n",
    "\n",
    "# Calculate time increment per frame in microseconds\n",
    "time_increment_per_frame = int((1 / frame_rate) * 1e6)\n",
    "\n",
    "# Generate the time vector\n",
    "time_vector = np.array([unix_time_start + i * time_increment_per_frame for i in range(total_frames)])\n",
    "\n",
    "# Display the first few elements of the time vector and its total length\n",
    "print(\"First few UNIX Times in Vector:\", time_vector[:5])\n",
    "print(\"Length of Time Vector:\", len(time_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the EMG and Kinematics\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_location = file_paths[\"File_Sync\"]  # Ensure the key matches exactly what you defined above\n",
    "\n",
    "# Base file location and code name\n",
    "# file_location = '/Users/beorn/Dropbox/0WORK/3Project Wensen/EMG Kin Research Experiment/Data/Experiment Data/SDC_007/EMG/2024-04-08_SDC_007_SD_Session1'\n",
    "codename = participant\n",
    "\n",
    "# Sensor names and file patterns\n",
    "RawSensorData = [\n",
    "    ('MagSensor_1', 'SDC_%s_Session1_Shimmer_7ED1_Calibrated_SD.dat'),\n",
    "    ('MagSensor_2', 'SDC_%s_Session1_Shimmer_7EDB_Calibrated_SD.dat'),\n",
    "    ('Right_Arm', 'SDC_%s_Session1_Shimmer_8AAA_Calibrated_SD.dat'),\n",
    "    ('Left_Arm', 'SDC_%s_Session1_Shimmer_8009_Calibrated_SD.dat'),\n",
    "    ('Supinator_Flex_Carp_Uln_R', 'SDC_%s_Session1_Shimmer_88F1_Calibrated_SD.dat'),\n",
    "    ('Pron_Teres_pron_quad_R', 'SDC_%s_Session1_Shimmer_88F4_Calibrated_SD.dat'),\n",
    "    ('Supinator_Flex_Carp_Uln_L', 'SDC_%s_Session1_Shimmer_88F6_Calibrated_SD.dat'),\n",
    "    ('Pron_Teres_pron_quad_L', 'SDC_%s_Session1_Shimmer_8FCF_Calibrated_SD.dat'),\n",
    "    ('Putter_Sensor', 'SDC_%s_Session1_Shimmer_541A_Calibrated_SD.dat'),\n",
    "]\n",
    "\n",
    "\n",
    "# for if the files happen to be CSV\n",
    "# RawSensorData = [\n",
    "#     ('MagSensor_1', 'SDC_%s_Session1_Shimmer_7ED1_Calibrated_SD.csv'),\n",
    "#     ('MagSensor_2', 'SDC_%s_Session1_Shimmer_7EDB_Calibrated_SD.csv'),\n",
    "#     ('Right_Arm', 'SDC_%s_Session1_Shimmer_8AAA_Calibrated_SD.csv'),\n",
    "#     ('Left_Arm', 'SDC_%s_Session1_Shimmer_8009_Calibrated_SD.csv'),\n",
    "#     ('Supinator_Flex_Carp_Uln_R', 'SDC_%s_Session1_Shimmer_88F1_Calibrated_SD.csv'),\n",
    "#     ('Pron_Teres_pron_quad_R', 'SDC_%s_Session1_Shimmer_88F4_Calibrated_SD.csv'),\n",
    "#     ('Supinator_Flex_Carp_Uln_L', 'SDC_%s_Session1_Shimmer_88F6_Calibrated_SD.csv'),\n",
    "#     ('Pron_Teres_pron_quad_L', 'SDC_%s_Session1_Shimmer_8FCF_Calibrated_SD.csv'),\n",
    "#     ('Putter_Sensor', 'SDC_%s_Session1_Shimmer_541A_Calibrated_SD.csv'),\n",
    "# ]\n",
    "\n",
    "# Initialize a list to keep track of the sensor names for which DataFrames are created\n",
    "loaded_sensors = []\n",
    "\n",
    "# Process each file based on the RawSensorData list\n",
    "for sensor_name, file_pattern in RawSensorData:\n",
    "    formatted_filename = file_pattern % codename\n",
    "    file_path = os.path.join(file_location, formatted_filename)\n",
    "    \n",
    "    # Attempt to read the file into a DataFrame and assign it to a global variable\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter='\\t', header=0, skiprows=[0, 1], dtype=float)\n",
    "        \n",
    "        # Adjust the 'ms' column to the desired format\n",
    "        if 'ms' in df.columns:\n",
    "            df['ms'] = (df['ms'] * 1e3).astype(np.int64)\n",
    "        \n",
    "        globals()[sensor_name] = df\n",
    "        loaded_sensors.append(sensor_name)  # Keep track of the loaded sensor names\n",
    "        print(f\"{sensor_name} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {sensor_name} due to: {e}\")\n",
    "\n",
    "# Now, `loaded_sensors` contains the names of all sensors for which DataFrames have been successfully loaded\n",
    "# You can access any DataFrame using globals(), e.g., globals()['MagSensor_1']\n",
    "\n",
    "# Cleanup\n",
    "del codename, file_location, file_path, file_pattern, formatted_filename, sensor_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for and replace corrupted time axis data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def check_and_fix_time(df):\n",
    "    \"\"\"\n",
    "    Function to fix repeated time points in the DataFrame by creating synthetic time points.\n",
    "    \"\"\"\n",
    "    total_observations = df.shape[0]\n",
    "\n",
    "    # Find the first repeating time point\n",
    "    unique_times, counts = np.unique(df.iloc[:, 0], return_counts=True)\n",
    "    first_repeating_index = np.where(counts > 1)[0]\n",
    "\n",
    "    if len(first_repeating_index) == 0:\n",
    "        print('No repeating time points found.')\n",
    "        return df\n",
    "    else:\n",
    "        print('One of the time axes has duplicates.')\n",
    "        first_repeating_time = unique_times[first_repeating_index[0]]\n",
    "\n",
    "        # Index of the first repeating time in the original DataFrame\n",
    "        start_index = df.index[df.iloc[:, 0] == first_repeating_time][0]\n",
    "\n",
    "        # Calculate average time interval based on the data before the first repeat\n",
    "        time_intervals = np.diff(df.iloc[:start_index, 0])\n",
    "        average_interval = np.mean(time_intervals)\n",
    "\n",
    "        # Number of synthetic points to generate\n",
    "        synthetic_points = total_observations - start_index\n",
    "\n",
    "        # Generate synthetic time points\n",
    "        start_time = df.iloc[start_index - 1, 0]\n",
    "        synthetic_times = start_time + (np.arange(1, synthetic_points + 1) * average_interval)\n",
    "\n",
    "        # Replace the repeating entries in the original DataFrame\n",
    "        df.iloc[start_index:, 0] = synthetic_times\n",
    "\n",
    "        return df\n",
    "\n",
    "# Now integrate this function with your existing code\n",
    "loaded_sensors = []\n",
    "\n",
    "for sensor_name, file_pattern in RawSensorData:\n",
    "    formatted_filename = file_pattern % participant\n",
    "    file_path = os.path.join(file_paths[\"File_Sync\"], formatted_filename)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter='\\t', header=0, skiprows=[0, 1], dtype=float)\n",
    "        \n",
    "        if 'ms' in df.columns:\n",
    "            df['ms'] = (df['ms'] * 1e3).astype(np.int64)\n",
    "        \n",
    "        # Fix time axis if necessary\n",
    "        df = check_and_fix_time(df)\n",
    "        \n",
    "        globals()[sensor_name] = df\n",
    "        loaded_sensors.append(sensor_name)\n",
    "        print(f\"{sensor_name} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {sensor_name} due to: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolating sensors to have the same time axis\n",
    "\n",
    "# Assuming the reference DataFrame for 'MagSensor_1' is already loaded and defined\n",
    "reference_df = globals()['MagSensor_1']\n",
    "reference_time_axis = reference_df.iloc[:, 0]  # Assuming the first column is UNIX time\n",
    "\n",
    "# Loop through each sensor DataFrame, interpolate it against the reference time axis, and update the global namespace\n",
    "for sensor_name, _ in RawSensorData:\n",
    "    if sensor_name == 'MagSensor_1':\n",
    "        # Optionally, you could also store the reference DataFrame with an 'interpolated' prefix if needed\n",
    "        globals()['interpolated_' + sensor_name] = reference_df\n",
    "        continue  # Skip interpolation for the reference sensor itself\n",
    "    \n",
    "    # Retrieve the current DataFrame\n",
    "    current_df = globals()[sensor_name]\n",
    "    \n",
    "    # Assuming the first column of each DataFrame is UNIX time, align and interpolate\n",
    "    current_df_indexed = current_df.set_index(current_df.columns[0])\n",
    "    aligned_df = current_df_indexed.reindex(reference_time_axis, method='nearest').interpolate(method='linear')\n",
    "    aligned_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Update the global namespace with the interpolated DataFrame\n",
    "    interpolated_name = 'interpolated_' + sensor_name\n",
    "    globals()[interpolated_name] = aligned_df\n",
    "\n",
    "    # Optionally print a success message\n",
    "    print(f\"{interpolated_name} created and stored successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a container to store data retrieve the unix kinematic and video time vector\n",
    "import pandas as pd\n",
    "\n",
    "class DataFrameContainer:\n",
    "    def __init__(self):\n",
    "        self.frames = {}\n",
    "    \n",
    "    def add_frame(self, name, dataframe):\n",
    "        \"\"\"Add a DataFrame to the container.\"\"\"\n",
    "        self.frames[name] = dataframe\n",
    "    \n",
    "    def get_frame(self, name):\n",
    "        \"\"\"Retrieve a DataFrame by name.\"\"\"\n",
    "        return self.frames.get(name, None)\n",
    "    \n",
    "    def get_all_frames(self):\n",
    "        \"\"\"Retrieve all stored DataFrames.\"\"\"\n",
    "        return self.frames\n",
    "    \n",
    "    def get_frame_names(self):\n",
    "        \"\"\"Retrieve the names of all stored DataFrames.\"\"\"\n",
    "        return list(self.frames.keys())\n",
    "\n",
    "# Create an instance of the container\n",
    "Sensor_Container = DataFrameContainer()\n",
    "\n",
    "# Assuming you have DataFrames named like 'interpolated_MagSensor_1', 'interpolated_MagSensor_2', etc.\n",
    "# Loop through the global namespace to find and add them to the container\n",
    "for var_name, var_value in list(globals().items()):\n",
    "    if isinstance(var_value, pd.DataFrame) and var_name.startswith('interpolated_'):\n",
    "        Sensor_Container.add_frame(var_name, var_value)\n",
    "\n",
    "# Assuming time_vector is an array-like structure with UNIX timestamps\n",
    "time_vector_df = pd.DataFrame(time_vector, columns=['UNIX Time'])\n",
    "# Adding the time_vector_df to the Sensor_Container\n",
    "Sensor_Container.add_frame('time_vector', time_vector_df)\n",
    "\n",
    "# Now, you can retrieve the names of all stored DataFrames using the new method\n",
    "frame_names = Sensor_Container.get_frame_names()\n",
    "print(\"Stored DataFrame names:\", frame_names)\n",
    "\n",
    "# Assuming 'Sensor_Container' is an object storing all your DataFrames, including 'time_vector' and 'interpolated_MagSensor_1'\n",
    "# Retrieve the 'UNIX Time' column from 'time_vector' DataFrame\n",
    "video_time_vector_unix = Sensor_Container.get_frame('time_vector')['UNIX Time'].values\n",
    "# Retrieve the 'ms' column from 'interpolated_MagSensor_1' DataFrame\n",
    "kinematics_time_vector_unix = Sensor_Container.get_frame('interpolated_Right_Arm')['ms'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a dataframe dictionary - dict - to retrieve desired vectors\n",
    "# List of DataFrame names you're interested in\n",
    "dataframe_names = [\n",
    "    'interpolated_MagSensor_1', 'interpolated_MagSensor_2', 'interpolated_Right_Arm',\n",
    "    'interpolated_Left_Arm', 'interpolated_Supinator_Flex_Carp_Uln_R',\n",
    "    'interpolated_Pron_Teres_pron_quad_R', 'interpolated_Supinator_Flex_Carp_Uln_L',\n",
    "    'interpolated_Pron_Teres_pron_quad_L', 'interpolated_Putter_Sensor'\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store the DataFrames\n",
    "Sensor_Dict = {}\n",
    "\n",
    "# Iterate over the list of DataFrame names\n",
    "for df_name in dataframe_names:\n",
    "    # Assuming each DataFrame name corresponds to a variable in the global namespace\n",
    "    # The globals() function is used to access these variables dynamically\n",
    "    Sensor_Dict[df_name] = globals()[df_name]\n",
    "\n",
    "# At this point, dataframes_dict contains all the specified DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a bandstop filter for the acceleration signals in the right arm \n",
    "import numpy as np\n",
    "from scipy.signal import butter, sosfilt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming Sensor_Dict is your dictionary with sensor data\n",
    "# Define sensor names and columns of interest\n",
    "sensor_names = ['interpolated_Right_Arm', 'interpolated_Left_Arm']\n",
    "columns = ['m/(s^2)', 'm/(s^2).1', 'm/(s^2).2']\n",
    "\n",
    "# Sampling rate and filter parameters\n",
    "fs = 512  # Sampling rate in Hz\n",
    "\n",
    "# Design the bandstop filters\n",
    "sos_1 = butter(N=4, Wn=[20, 85], btype='bandstop', fs=fs, output='sos')\n",
    "sos_2 = butter(N=4, Wn=[100, 240], btype='bandstop', fs=fs, output='sos')\n",
    "\n",
    "# Apply the filters to the data\n",
    "for sensor in sensor_names:\n",
    "    if sensor in Sensor_Dict:\n",
    "        for column in columns:\n",
    "            # Apply filters sequentially directly on Sensor_Dict\n",
    "            Sensor_Dict[sensor][column] = sosfilt(sos_1, Sensor_Dict[sensor][column])\n",
    "            Sensor_Dict[sensor][column] = sosfilt(sos_2, Sensor_Dict[sensor][column])\n",
    "\n",
    "# Example plot for one sensor and column\n",
    "sensor_example = 'interpolated_Right_Arm'\n",
    "column_example = 'm/(s^2)'\n",
    "\n",
    "# # ______HERE IS FOR PLOTTING TO SEE HOW WE ARE DOING________\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# # Note: Since Sensor_Dict is now altered, plotting \"Original\" would actually plot the filtered data\n",
    "# # To plot original vs. filtered comparison, you would need to save the original data before filtering\n",
    "# plt.plot(Sensor_Dict[sensor_example].index, Sensor_Dict[sensor_example][column_example], label='Filtered', alpha=0.5, linestyle='--')\n",
    "# plt.legend()\n",
    "# plt.title(f\"{sensor_example} - {column_example}: Filtered\")\n",
    "# plt.xlabel('Sample Index')\n",
    "# plt.ylabel('Acceleration (m/s^2)')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match unix time stamps from video with the corresponding closest values from the kineamtics\n",
    "import numpy as np\n",
    "\n",
    "def find_closest_indices(main_array, reference_array):\n",
    "    \"\"\"\n",
    "    For each timestamp in the main_array, find the index of the closest timestamp in the reference_array.\n",
    "    \"\"\"\n",
    "    indices = np.zeros(len(main_array), dtype=np.int64)\n",
    "    for i, value in enumerate(main_array):\n",
    "        indices[i] = np.argmin(np.abs(reference_array - value))\n",
    "    return indices\n",
    "\n",
    "def synchronize_datasets(video_time_vector_unix, kinematics_time_vector_unix):\n",
    "    # Identify the overlapping period\n",
    "    overlap_start = max(video_time_vector_unix[0], kinematics_time_vector_unix[0])\n",
    "    overlap_end = min(video_time_vector_unix[-1], kinematics_time_vector_unix[-1])\n",
    "    \n",
    "    # Filter the kinematics timestamps to the overlapping period\n",
    "    kinematics_indices = np.where((kinematics_time_vector_unix >= overlap_start) & (kinematics_time_vector_unix <= overlap_end))[0]\n",
    "    kinematics_overlap_timestamps = kinematics_time_vector_unix[kinematics_indices]\n",
    "    \n",
    "    # Filter the video timestamps to the overlapping period\n",
    "    video_indices = np.where((video_time_vector_unix >= overlap_start) & (video_time_vector_unix <= overlap_end))[0]\n",
    "    video_overlap_timestamps = video_time_vector_unix[video_indices]\n",
    "    \n",
    "    # Find the closest kinematics timestamps to each video timestamp\n",
    "    closest_kinematics_indices = find_closest_indices(video_overlap_timestamps, kinematics_overlap_timestamps)\n",
    "    \n",
    "    # Prepare the output array with additional columns\n",
    "    # Column 0: Aligned index in kinematics_time_vector_unix\n",
    "    # Column 1: Corresponding kinematics timestamp\n",
    "    # Column 2: Aligned index in video_time_vector_unix (for reference)\n",
    "    # Column 3: Corresponding video timestamp\n",
    "    aligned_data = np.zeros((len(video_indices), 3), dtype=np.int64)\n",
    "    aligned_data[:, 0] = kinematics_indices[closest_kinematics_indices]\n",
    "    aligned_data[:, 1] = kinematics_time_vector_unix[aligned_data[:, 0]]\n",
    "    aligned_data[:, 2] = video_time_vector_unix[video_indices]\n",
    "    \n",
    "    return aligned_data\n",
    "\n",
    "# Assuming your time vectors are named kinematics_time_vector_unix and video_time_vector_unix\n",
    "aligned_data = synchronize_datasets(video_time_vector_unix, kinematics_time_vector_unix)\n",
    "\n",
    "# The aligned_data array now contains the aligned indices and their corresponding timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the synced selected kinematic and EMG values camera_sycned_sensors\n",
    "Camera_Synced_Sensors = {}\n",
    "\n",
    "# Extract the indices from the first column of aligned_data\n",
    "indices = aligned_data[:, 0]\n",
    "\n",
    "# Iterate over each item in Sensor_Dict\n",
    "for name, df in Sensor_Dict.items():\n",
    "    # Check if the name starts with 'interpolated_'\n",
    "    if name.startswith('interpolated_'):\n",
    "        # Select rows from the DataFrame based on the extracted indices\n",
    "        # Note: Ensure that the indices are valid for the DataFrame\n",
    "        filtered_df = df.iloc[indices]\n",
    "        \n",
    "        # Store the filtered DataFrame in the new dictionary\n",
    "        Camera_Synced_Sensors[name] = filtered_df\n",
    "\n",
    "# At this point, 'filtered_dataframes' contains all the filtered DataFrames\n",
    "# from Sensor_Dict that start with 'interpolated_', with rows corresponding to the indices in aligned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot decimated kin/emg \n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming Camera_Synced_Sensors is a dictionary with your DataFrames\n",
    "\n",
    "# Access the DataFrames directly from the Camera_Synced_Sensors dictionary\n",
    "interpolated_MagSensor_1 = Camera_Synced_Sensors['interpolated_MagSensor_1']\n",
    "interpolated_MagSensor_2 = Camera_Synced_Sensors['interpolated_MagSensor_2']\n",
    "interpolated_Right_Arm = Camera_Synced_Sensors['interpolated_Right_Arm']\n",
    "interpolated_Left_Arm = Camera_Synced_Sensors['interpolated_Left_Arm']\n",
    "interpolated_Putter_Sensor = Camera_Synced_Sensors['interpolated_Putter_Sensor']\n",
    "\n",
    "\n",
    "\n",
    "# Assuming the first column is the time axis for all sensors\n",
    "time_axis = interpolated_MagSensor_1.iloc[:, 0]\n",
    "\n",
    "# Create the figure with shared x-axes\n",
    "fig = make_subplots(rows=5, cols=1, shared_xaxes=True)\n",
    "\n",
    "# Helper function to add traces for cleanliness\n",
    "def add_trace(fig, df, row, sensor_name):\n",
    "    data_series = df.iloc[:, 3]  # Assuming the second column is what you want to plot\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=data_series, mode='lines', name=sensor_name), row=row, col=1)\n",
    "\n",
    "# Add traces for each sensor\n",
    "add_trace(fig, interpolated_MagSensor_1, 1, 'Interpolated MagSensor 1')\n",
    "add_trace(fig, interpolated_MagSensor_2, 2, 'Interpolated MagSensor 2')\n",
    "add_trace(fig, interpolated_Right_Arm, 3, 'Interpolated Right Arm')\n",
    "add_trace(fig, interpolated_Left_Arm, 4, 'Interpolated Left Arm')\n",
    "add_trace(fig, interpolated_Putter_Sensor, 5, 'Interpolated Putter Sensor')\n",
    "\n",
    "\n",
    "# Update layout and set the title\n",
    "fig.update_layout(height=1200, width=1000, title_text=\"Comparison of Interpolated Sensors Time Series\")\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"Sensor Data\")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot full kin/emg and markers of camera start stop \n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Access the DataFrames directly from the Sensor_Dict dictionary\n",
    "interpolated_MagSensor_1 = Sensor_Dict['interpolated_MagSensor_1']\n",
    "interpolated_MagSensor_2 = Sensor_Dict['interpolated_MagSensor_2']\n",
    "interpolated_Right_Arm = Sensor_Dict['interpolated_Right_Arm']\n",
    "interpolated_Left_Arm = Sensor_Dict['interpolated_Left_Arm']\n",
    "interpolated_Putter_Sensor = Sensor_Dict['interpolated_Putter_Sensor']\n",
    "\n",
    "# Assuming the first column is the time axis for all sensors\n",
    "time_axis = interpolated_MagSensor_1.iloc[:, 0]\n",
    "\n",
    "# Create the figure with shared x-axes\n",
    "fig = make_subplots(rows=5, cols=1, shared_xaxes=True, subplot_titles=('Interpolated MagSensor 1', 'Interpolated MagSensor 2', 'Interpolated Right Arm', 'Interpolated Left Arm', 'Interpolated Putter Sensor'))\n",
    "\n",
    "# Helper function to add traces for cleanliness\n",
    "def add_trace(fig, df, row, sensor_name):\n",
    "    data_series = df.iloc[:, 1]  # Assuming the second column is what you want to plot\n",
    "    fig.add_trace(go.Scatter(x=time_axis, y=data_series, mode='lines', name=sensor_name), row=row, col=1)\n",
    "\n",
    "# Add traces for each sensor\n",
    "add_trace(fig, interpolated_MagSensor_1, 1, 'Interpolated MagSensor 1')\n",
    "add_trace(fig, interpolated_MagSensor_2, 2, 'Interpolated MagSensor 2')\n",
    "add_trace(fig, interpolated_Right_Arm, 3, 'Interpolated Right Arm')\n",
    "add_trace(fig, interpolated_Left_Arm, 4, 'Interpolated Left Arm')\n",
    "add_trace(fig, interpolated_Putter_Sensor, 5, 'Interpolated Putter Sensor')\n",
    "\n",
    "# Assuming aligned_data is your ndarray and the first column contains the indices\n",
    "first_index = aligned_data[0, 0]  # First observation index in aligned_data\n",
    "last_index = aligned_data[-1, 0]  # Last observation index in aligned_data\n",
    "\n",
    "# Get the time values for the first and last observation\n",
    "first_time = time_axis.iloc[first_index]\n",
    "last_time = time_axis.iloc[last_index]\n",
    "\n",
    "# Function to add vertical lines\n",
    "def add_vertical_line(fig, x, row, color='red'):\n",
    "    fig.add_shape(type='line', \n",
    "                  x0=x, y0=0, x1=x, y1=1, \n",
    "                  xref='x'+str(row), yref='paper',\n",
    "                  line=dict(color=color, width=2),\n",
    "                  row=row, col=1)\n",
    "\n",
    "# Add vertical lines for the first and last observation across all subplots\n",
    "for i in range(1, 6):  # Adjust the range according to the number of subplots\n",
    "    add_vertical_line(fig, first_time, i)\n",
    "    add_vertical_line(fig, last_time, i)\n",
    "\n",
    "# Update layout and set the title\n",
    "fig.update_layout(height=1500, width=1000, title_text=\"Comparison of Interpolated Sensors Time Series\")\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"Sensor Data\")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variable names to keep \n",
    "variables_to_keep = ['Camera_Synced_Sensors', 'aligned_data', 'Sensor_Dict', 'file_paths', 'video_paths', 'write_paths', 'save_paths', 'participant', 'condition', 'hand_type','aligned_data_save']\n",
    "# Iterate over the list of variables in the global namespace\n",
    "for variable in list(globals()):\n",
    "    # If the variable is not in the list to keep, delete it\n",
    "    if variable not in variables_to_keep:\n",
    "        del globals()[variable]\n",
    "# Now, only the variables in 'variables_to_keep' should remain\n",
    "del variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FOR MAC ############## Engine to input data for perfect syncing video to EMG###############\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from IPython.display import display, clear_output\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.graph_objs import FigureWidget\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntText\n",
    "\n",
    "# Data loading section\n",
    "# Load your data\n",
    "time_series_data_1 = Camera_Synced_Sensors.get('interpolated_MagSensor_1')['local_flux'].values\n",
    "time_series_data_2 = Camera_Synced_Sensors.get('interpolated_Putter_Sensor')['m/(s^2).1'].values\n",
    "\n",
    "# Define the paths to your video files using the video_paths dictionary\n",
    "video_file_path = video_paths[\"Videopanel_Sync\"]  # Ensure the key matches exactly what you defined above\n",
    "\n",
    "# Initialize a global list to store click coordinates or frame indices\n",
    "clicked_points = []\n",
    "# Initialize a list to store the values\n",
    "offset_window_info = []\n",
    "\n",
    "# Video capture setup\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "window_width = 15000\n",
    "start_frame = 1\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Initialize the input widget for offset value but do not display it yet\n",
    "offset_input = IntText(\n",
    "    value=0,\n",
    "    description='Set Offset:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Right after defining offset_input\n",
    "confirmed_offset = 0\n",
    "\n",
    "# Define the function that updates confirmed_offset based on the widget's input\n",
    "def update_confirmed_offset(change):\n",
    "    global confirmed_offset\n",
    "    confirmed_offset = change['new'] \n",
    "    print(\"Observer triggered, confirmed_offset updated to:\", confirmed_offset)\n",
    "\n",
    "# Attach the observer to the offset_input widget\n",
    "offset_input.observe(update_confirmed_offset, names='value')\n",
    "\n",
    "pause = False\n",
    "\n",
    "# Function to convert matplotlib figure to an OpenCV image\n",
    "def plot_to_image(figure):\n",
    "    \"\"\"Convert a Matplotlib figure to an OpenCV image.\"\"\"\n",
    "    canvas = FigureCanvas(figure)\n",
    "    canvas.draw()\n",
    "    img = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
    "    img = img.reshape(figure.canvas.get_width_height()[::-1] + (3,))\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "def display_interactive_graph(window_data_1, window_data_2, current_frame, total_frames, y_min_1, y_max_1, y_min_2, y_max_2):\n",
    "    clear_output(wait=True)  # Clear the previous output, including Plotly graphs \n",
    "    # Create an X-axis that spans the entire range of the video/data\n",
    "    full_x_values = list(range(window_start, window_end))  # Adjust to show the correct range\n",
    "    \n",
    "    # Creating subplots\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,\n",
    "                        subplot_titles=(\"Sensor 1 Data\", \"Sensor 2 Data\"))\n",
    "    \n",
    "    # Adding traces to the subplots\n",
    "    fig.add_trace(go.Scatter(x=full_x_values, y=window_data_1, mode='lines+markers', name='Sensor 1'),\n",
    "                  row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=full_x_values, y=window_data_2, mode='lines+markers', name='Sensor 2', line=dict(color='red')),\n",
    "                  row=2, col=1)\n",
    "    \n",
    "    # Adjust the view to zoom in on the current window around the pause point\n",
    "    fig.update_layout(\n",
    "        title_text=\"Sensor Data vs. Video Frame\",\n",
    "        xaxis_title=\"Video Frame Index\",\n",
    "        yaxis1=dict(range=[y_min_1, y_max_1]),\n",
    "        yaxis2=dict(range=[y_min_2, y_max_2]),\n",
    "        xaxis2=dict(title=\"Video Frame Index\", range=[current_frame - window_width // 2, current_frame + window_width // 2])\n",
    "    )\n",
    "    \n",
    "    # Add a vertical red line at the current frame (midpoint) for both subplots\n",
    "    for i in range(2):\n",
    "        fig.add_shape(\n",
    "            # Line Vertical\n",
    "            dict(\n",
    "                type=\"line\",\n",
    "                x0=current_frame,\n",
    "                y0=min(y_min_1, y_min_2),\n",
    "                x1=current_frame,\n",
    "                y1=max(y_max_1, y_max_2),\n",
    "                line=dict(\n",
    "                    color=\"Red\",\n",
    "                    width=3\n",
    "                )\n",
    "            ),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "\n",
    "    # Event handler for capturing clicks on the graph\n",
    "    def on_click(trace, points, selector):\n",
    "        for point in points.point_inds:\n",
    "            clicked_index = full_x_values[point]  # Get the x-value (frame index) of the clicked point\n",
    "            clicked_points.append(clicked_index)  # Save the clicked frame index\n",
    "            print(f\"Clicked on index: {clicked_index}\")  # Optional: Print or process the clicked index as needed\n",
    "\n",
    "    # Attach the click event handler to the figure\n",
    "    scatter = fig.data[0]\n",
    "    scatter.on_click(on_click)\n",
    "\n",
    "    display(fig)  # Use IPython's display to show the fig\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "while True:\n",
    "    if not pause:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES)) - 1\n",
    "\n",
    "    # Use confirmed_offset in the calculation\n",
    "    window_start = max(0, current_frame - window_width // 2 + confirmed_offset)\n",
    "    window_end = min(len(time_series_data_1), current_frame + window_width // 2 + 1 + confirmed_offset)\n",
    "    window_data_1 = time_series_data_1[window_start:window_end]\n",
    "    window_start_2 = max(0, current_frame - window_width // 2 + confirmed_offset)\n",
    "    window_end_2 = min(len(time_series_data_2), current_frame + window_width // 2 + 1 + confirmed_offset)\n",
    "    window_data_2 = time_series_data_2[window_start_2:window_end_2]\n",
    "    \n",
    "\n",
    "    if pause:\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('p'):\n",
    "            pause = False\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Before your loop or inside it before plotting, calculate the middle index relative to the window_data array\n",
    "    middle_index = len(window_data_1) // 2  \n",
    "    ax[0].clear()\n",
    "    ax[0].plot(window_data_1)\n",
    "    ax[0].axvline(x=middle_index, color='r', linestyle='--')  # Adds a vertical red line at the middle of the plot\n",
    "    ax[0].set_title('Sensor 1 Data')\n",
    "    ax[1].clear()\n",
    "    ax[1].plot(window_data_2, color='red')\n",
    "    ax[1].axvline(x=middle_index, color='r', linestyle='--')  # Adds a vertical red line at the middle of the plot\n",
    "    ax[1].set_title('Sensor 2 Data')\n",
    "    fig.canvas.draw()\n",
    "    plot_image = plot_to_image(fig)\n",
    "\n",
    "    # Resize plot image to fit on video frame and adjust position to top left corner\n",
    "    plot_image_resized = cv2.resize(plot_image, (int(frame.shape[1] * 0.25), int(frame.shape[0] * 0.5)))\n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "    frame[y_offset:y_offset+plot_image_resized.shape[0], x_offset:x_offset+plot_image_resized.shape[1]] = plot_image_resized\n",
    "\n",
    "    cv2.imshow('Frame with Time Series', frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('p'):\n",
    "        pause = not pause  # Toggle the pause state\n",
    "        if pause:\n",
    "            # Logic when pausing\n",
    "            y_min_1 = min(window_data_1)\n",
    "            y_max_1 = max(window_data_1)\n",
    "            y_min_2 = min(window_data_2)\n",
    "            y_max_2 = max(window_data_2)\n",
    "\n",
    "            # Display the Plotly graph for consulting\n",
    "            display_interactive_graph(window_data_1, window_data_2, current_frame, total_frames, y_min_1, y_max_1, y_min_2, y_max_2)\n",
    "\n",
    "            # Then, display the manual input widget for the offset value\n",
    "            display(offset_input)\n",
    "        else:\n",
    "            # Logic when resuming from pause\n",
    "            # Update confirmed_offset with the value entered during pause\n",
    "            confirmed_offset = offset_input.value\n",
    "            # Here, you might need to apply the confirmed_offset in adjusting your data display logic\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup remains unchanged\n",
    "cap.release()\n",
    "cv2.waitKey(1)  # Sometimes helps with window closing\n",
    "cv2.destroyAllWindows()\n",
    "cv2.destroyWindow('Frame with Time Series')\n",
    "plt.close(fig)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FOR PC ############# Engine to input data for perfect syncing video to EMG###############\n",
    "# %matplotlib widget\n",
    "# import cv2\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "# from IPython.display import display, clear_output\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.graph_objs import FigureWidget\n",
    "# import ipywidgets as widgets\n",
    "# from ipywidgets import IntText  # Import IntText explicitly\n",
    "\n",
    "\n",
    "# # Data loading section\n",
    "# # Load your data\n",
    "# time_series_data = Camera_Synced_Sensors.get('interpolated_MagSensor_1')['local_flux'].values\n",
    "# # time_series_data = Camera_Synced_Sensors.get('interpolated_Putter_Sensor')['m/(s^2).1'].values\n",
    "\n",
    "# # # Video file path setup\n",
    "# # video_path = \"/Users/beorn/Dropbox/0WORK/3Project Wensen/EMG Kin Research Experiment/Data/Experiment Data/SDC_007/Videos/Videopanel/SDC_007_Ball_RH_VP_synced_time_injection.MP4\"\n",
    "\n",
    "# # Define the paths to your video files using the video_paths dictionary\n",
    "# video_file_path = video_paths[\"Videopanel_Sync\"]  # Ensure the key matches exactly what you defined above\n",
    "\n",
    "\n",
    "# # Initialize a global list to store click coordinates or frame indices\n",
    "# clicked_points = []\n",
    "# # Initialize a list to store the values\n",
    "# offset_window_info = []\n",
    "\n",
    "# # Video capture setup\n",
    "# cap = cv2.VideoCapture(video_file_path)\n",
    "# window_width = 5000\n",
    "# start_frame = 200\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "# total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# # Initialize the input widget for offset value but do not display it yet\n",
    "# offset_input = IntText(\n",
    "#     value=0,\n",
    "#     description='Set Offset:',\n",
    "#     disabled=False\n",
    "# )\n",
    "\n",
    "# # Right after defining offset_input\n",
    "# confirmed_offset = 0\n",
    "\n",
    "\n",
    "# # Define the function that updates confirmed_offset based on the widget's input\n",
    "# def update_confirmed_offset(change):\n",
    "#     global confirmed_offset\n",
    "#     confirmed_offset = change['new'] \n",
    "#     print(\"Observer triggered, confirmed_offset updated to:\", confirmed_offset)\n",
    " \n",
    "\n",
    "# # Attach the observer to the offset_input widget\n",
    "# offset_input.observe(update_confirmed_offset, names='value')\n",
    "\n",
    "# pause = False\n",
    "\n",
    "# # Function to convert matplotlib figure to an OpenCV image\n",
    "# def plot_to_image(figure):\n",
    "#     \"\"\"Convert a Matplotlib figure to an OpenCV image.\"\"\"\n",
    "#     canvas = FigureCanvas(figure)\n",
    "#     canvas.draw()\n",
    "#     img = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
    "#     img = img.reshape(figure.canvas.get_width_height()[::-1] + (3,))\n",
    "#     return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "\n",
    "# def display_interactive_graph(window_data, current_frame, total_frames, y_min, y_max):\n",
    "#     clear_output(wait=True)  # Clear the previous output, including Plotly graphs \n",
    "#     # Create an X-axis that spans the entire range of the video/data\n",
    "#     full_x_values = list(range(window_start, window_end))  # Adjust to show the correct range\n",
    "#     # Creating a FigureWidget for interactive plotting\n",
    "#     fig = FigureWidget([go.Scatter(x=full_x_values, y=window_data, mode='lines+markers')])\n",
    "#     # Adjust the view to zoom in on the current window around the pause point\n",
    "#     fig.update_layout(\n",
    "#         title_text=\"Sensor Data vs. Video Frame\",\n",
    "#         xaxis_title=\"Video Frame Index\",\n",
    "#         yaxis=dict(range=[y_min, y_max]),\n",
    "#         xaxis_range=[current_frame - window_width // 2, current_frame + window_width // 2]\n",
    "#     )\n",
    "#         # Add a vertical red line at the current frame (midpoint)\n",
    "#     fig.add_shape(\n",
    "#         # Line Vertical\n",
    "#         dict(\n",
    "#             type=\"line\",\n",
    "#             x0=current_frame,\n",
    "#             y0=y_min,\n",
    "#             x1=current_frame,\n",
    "#             y1=y_max,\n",
    "#             line=dict(\n",
    "#                 color=\"Red\",\n",
    "#                 width=3\n",
    "#             )\n",
    "#     ))\n",
    "\n",
    "#     # Event handler for capturing clicks on the graph\n",
    "#     def on_click(trace, points, selector):\n",
    "#         for point in points.point_inds:\n",
    "#             clicked_index = full_x_values[point]  # Get the x-value (frame index) of the clicked point\n",
    "#             clicked_points.append(clicked_index)  # Save the clicked frame index\n",
    "#             print(f\"Clicked on index: {clicked_index}\")  # Optional: Print or process the clicked index as needed\n",
    "#     # Attach the click event handler to the figure\n",
    "#     scatter = fig.data[0]\n",
    "#     scatter.on_click(on_click)\n",
    "\n",
    "#     display(fig)  # Use IPython's display to show the fig\n",
    "    \n",
    "\n",
    "# # Note: You might need to adjust the range calculation for `full_x_values` based on your specific needs.\n",
    "\n",
    "# # perhaps needed for something we don't need it for anymore\n",
    "# plt.ion()\n",
    "# fig, ax = plt.subplots()\n",
    "# # cid = fig.canvas.mpl_connect('button_press_event', on_click)\n",
    "\n",
    "\n",
    "# desired_width = 1280  # Change this to your desired width\n",
    "# desired_height = 720  # Change this to your desired height\n",
    "\n",
    "# while True:\n",
    "#     if not pause:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         # Resize the video frame to the desired dimensions\n",
    "#         frame = cv2.resize(frame, (desired_width, desired_height))\n",
    "#         current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES)) - 1\n",
    "\n",
    "#     # Use confirmed_offset in the calculation\n",
    "#     window_start = max(0, current_frame - window_width // 2 + confirmed_offset)\n",
    "#     window_end = min(len(time_series_data), current_frame + window_width // 2 + 1 + confirmed_offset)\n",
    "#     window_data = time_series_data[window_start:window_end]\n",
    "\n",
    "#     if pause:\n",
    "#         key = cv2.waitKey(1) & 0xFF\n",
    "#         if key == ord('p'):\n",
    "#             pause = False\n",
    "#         elif key == ord('q'):\n",
    "#             break\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#     middle_index = len(window_data) // 2  \n",
    "#     ax.clear()\n",
    "#     ax.plot(window_data)\n",
    "#     ax.axvline(x=middle_index, color='r', linestyle='--')  # Adds a vertical red line at the middle of the plot\n",
    "#     fig.canvas.draw()\n",
    "#     plot_image = plot_to_image(fig)\n",
    "\n",
    "#     # Resize plot image to fit on video frame and adjust position to top left corner\n",
    "#     plot_image_resized = cv2.resize(plot_image, (int(desired_width * 0.25), int(desired_height * 0.5)))\n",
    "#     x_offset = 0\n",
    "#     y_offset = 0\n",
    "#     frame[y_offset:y_offset+plot_image_resized.shape[0], x_offset:x_offset+plot_image_resized.shape[1]] = plot_image_resized\n",
    "\n",
    "#     cv2.imshow('Frame with Time Series', frame)\n",
    "\n",
    "#     key = cv2.waitKey(1) & 0xFF\n",
    "#     if key == ord('p'):\n",
    "#         pause = not pause\n",
    "#         if pause:\n",
    "#             y_min = min(window_data)\n",
    "#             y_max = max(window_data)\n",
    "#             display_interactive_graph(window_data, current_frame, total_frames, y_min, y_max)\n",
    "#             display(offset_input)\n",
    "#         else:\n",
    "#             confirmed_offset = offset_input.value\n",
    "#     elif key == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.waitKey(1)  # Sometimes helps with window closing\n",
    "# cv2.destroyAllWindows()  # This destroys all windows\n",
    "# plt.close(fig)  # Close the matplotlib plot if it's no longer needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Frame Correction and Starting Frame for video sync\n",
    "\n",
    "frame_correction = confirmed_offset-current_frame\n",
    "\n",
    "\n",
    "confirmed_start_frame = current_frame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct .json save\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load existing data from the JSON file\n",
    "def load_existing_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {\n",
    "            \"Ball\": {\"BH\": {\"data\": [], \"offset\": None},\n",
    "                     \"RH\": {\"data\": [], \"offset\": None},\n",
    "                     \"LH\": {\"data\": [], \"offset\": None}},\n",
    "            \"NB\": {\"BH\": {\"data\": [], \"offset\": None},\n",
    "                   \"RH\": {\"data\": [], \"offset\": None},\n",
    "                   \"LH\": {\"data\": [], \"offset\": None}}\n",
    "        }\n",
    "\n",
    "# Define the file path for the JSON file\n",
    "output_directory = os.path.dirname(aligned_data_save[\"aligned_data_path\"])\n",
    "output_file = os.path.basename(aligned_data_save[\"aligned_data_path\"])\n",
    "output_path = os.path.join(output_directory, output_file)\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Load existing data or initialize if not present\n",
    "data_to_save = load_existing_data(output_path)\n",
    "\n",
    "# Insert the aligned_data into the correct condition and hand type, converting ndarray to list\n",
    "data_to_save[condition][hand_type]['data'] = aligned_data.tolist()\n",
    "data_to_save[condition][hand_type]['offset'] = frame_correction  # Store the offset\n",
    "\n",
    "# Write the updated data to the JSON file\n",
    "with open(output_path, 'w') as json_file:\n",
    "    json.dump(data_to_save, json_file, indent=4)\n",
    "\n",
    "print(f\"Data successfully saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves Time axis To Matlab deployable file in the appropriate directory this one works\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "# Define the path to save the .mat file\n",
    "save_save_path = save_paths[\"Save_Sync\"]\n",
    "\n",
    "# Load the existing data from the .mat file if it exists, otherwise initialize it\n",
    "try:\n",
    "    loaded_data = scipy.io.loadmat(save_save_path, squeeze_me=True)\n",
    "    if 'Time_Axis_Alignment' in loaded_data:\n",
    "        time_axis_alignment = loaded_data['Time_Axis_Alignment']\n",
    "        existing_data = {}\n",
    "        for condition_key in ['Ball', 'NB']:\n",
    "            if condition_key in time_axis_alignment.dtype.names:\n",
    "                existing_data[condition_key] = {}\n",
    "                condition_data = time_axis_alignment[condition_key].item()\n",
    "                for hand_key in ['BH', 'RH', 'LH']:\n",
    "                    if hand_key in condition_data.dtype.names:\n",
    "                        existing_data[condition_key][hand_key] = condition_data[hand_key].item()\n",
    "                    else:\n",
    "                        existing_data[condition_key][hand_key] = ['no data', 'no data']\n",
    "            else:\n",
    "                existing_data[condition_key] = {\n",
    "                    'BH': ['no data', 'no data'],\n",
    "                    'RH': ['no data', 'no data'],\n",
    "                    'LH': ['no data', 'no data']\n",
    "                }\n",
    "    else:\n",
    "        raise KeyError(\"Time_Axis_Alignment not found in loaded data.\")\n",
    "except (FileNotFoundError, KeyError):\n",
    "    existing_data = {\n",
    "        'Ball': {\n",
    "            'BH': ['no data', 'no data'],\n",
    "            'RH': ['no data', 'no data'],\n",
    "            'LH': ['no data', 'no data']\n",
    "        },\n",
    "        'NB': {\n",
    "            'BH': ['no data', 'no data'],\n",
    "            'RH': ['no data', 'no data'],\n",
    "            'LH': ['no data', 'no data']\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Ask the user which hand configuration to update\n",
    "#choice = input(\"Choose the hand configuration to update (BH, RH, LH): \").strip().upper()\n",
    "# Use the global variable for hand configuration\n",
    "choice = hand_type\n",
    "\n",
    "# Prepare the data to be saved\n",
    "aligned_data_double = aligned_data.astype(np.float64)\n",
    "frame_correction_double = np.array([frame_correction], dtype=np.float64)\n",
    "\n",
    "# Update only the chosen hand configuration and condition in the existing data\n",
    "if choice in existing_data[condition]:\n",
    "    if choice == 'RH':\n",
    "        existing_data[condition][choice] = [[aligned_data_double, frame_correction_double]]\n",
    "    else:\n",
    "        existing_data[condition][choice] = [aligned_data_double, frame_correction_double]\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(os.path.dirname(save_save_path), exist_ok=True)\n",
    "\n",
    "# Prepare the nested structure for saving\n",
    "time_axis_alignment_struct = {\n",
    "    condition_key: {\n",
    "        hand_key: np.array(existing_data[condition_key][hand_key], dtype=object)\n",
    "        for hand_key in existing_data[condition_key]\n",
    "    } for condition_key in existing_data\n",
    "}\n",
    "\n",
    "# Save the updated data to a .mat file\n",
    "scipy.io.savemat(save_save_path, {\n",
    "    'Time_Axis_Alignment': time_axis_alignment_struct\n",
    "})\n",
    "\n",
    "print(f\"Data saved for {choice} under condition {condition} in the 'Time Axis Alignment' structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Test accuracy of syncing with Video playback, record video for posterity \n",
    "\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "# # Load your time series data\n",
    "# time_series_data1 = Camera_Synced_Sensors.get('interpolated_MagSensor_1')['local_flux'].values\n",
    "# time_series_data2 = Camera_Synced_Sensors.get('interpolated_MagSensor_2')['local_flux'].values\n",
    "# time_series_data3 = Camera_Synced_Sensors.get('interpolated_Right_Arm')['m/(s^2)'].values\n",
    "# time_series_data4 = Camera_Synced_Sensors.get('interpolated_Right_Arm')['m/(s^2).1'].values\n",
    "# time_series_data5 = Camera_Synced_Sensors.get('interpolated_Right_Arm')['m/(s^2).2'].values\n",
    "# time_series_data6 = Camera_Synced_Sensors.get('interpolated_Putter_Sensor')['m/(s^2).2'].values\n",
    "# time_series_data7 = Camera_Synced_Sensors.get('interpolated_Supinator_Flex_Carp_Uln_R')['mV'].values\n",
    "# time_series_data8 = Camera_Synced_Sensors.get('interpolated_Pron_Teres_pron_quad_R')['mV'].values\n",
    "# time_series_data9 = Camera_Synced_Sensors.get('interpolated_Pron_Teres_pron_quad_R')['mV.1'].values\n",
    "# time_series_data10 = Camera_Synced_Sensors.get('interpolated_Supinator_Flex_Carp_Uln_L')['mV'].values\n",
    "\n",
    "# # Path to your video file\n",
    "# video_file_path = video_paths[\"Videopanel_Sync\"]  # Ensure this key matches your variable definition\n",
    "# write_file_path = write_paths[\"Write_Sync\"]  # Ensure this key matches your variable definition\n",
    "\n",
    "# # Window settings\n",
    "# window_width = 200  # Width of the time-series window to display\n",
    "# start_frame = confirmed_start_frame - 50  # Adjust start frame to capture earlier data\n",
    "\n",
    "# # Initialize OpenCV VideoCapture\n",
    "# cap = cv2.VideoCapture(video_file_path)\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "# current_frame = start_frame\n",
    "\n",
    "# # Function to convert matplotlib figure to OpenCV image\n",
    "# def plot_to_image(figure):\n",
    "#     canvas = FigureCanvas(figure)\n",
    "#     canvas.draw()\n",
    "#     img = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
    "#     img = img.reshape(figure.canvas.get_width_height()[::-1] + (3,))\n",
    "#     return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# # Setting up the video writer with the correct codec\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' for MP4 files\n",
    "# out_frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# out_frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# out_video = cv2.VideoWriter(write_file_path, fourcc, 30.0, (out_frame_width, out_frame_height))\n",
    "\n",
    "# # Interactive plot setup\n",
    "# plt.ion()\n",
    "# fig, axs = plt.subplots(10, 1, figsize=(5, 7))  # Adjust subplots if needed\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     current_frame += 1\n",
    "\n",
    "#     # For each time series, calculate and plot the window\n",
    "#     for i, time_series_data in enumerate([\n",
    "#         time_series_data1, time_series_data2, time_series_data3, time_series_data4,\n",
    "#         time_series_data5, time_series_data6, time_series_data7, time_series_data8,\n",
    "#         time_series_data9, time_series_data10\n",
    "#     ]):\n",
    "#         window_start = max(0, current_frame - window_width // 2 + frame_correction)\n",
    "#         window_end = min(len(time_series_data), current_frame + window_width // 2 + 1 + frame_correction)\n",
    "#         window_data = time_series_data[window_start:window_end]\n",
    "\n",
    "#         # Plot each subplot\n",
    "#         axs[i].clear()\n",
    "#         axs[i].plot(window_data)\n",
    "#         middle_index = len(window_data) // 2\n",
    "#         axs[i].axvline(x=middle_index, color='r', linestyle='--')  # Middle red vertical line\n",
    "    \n",
    "#     fig.canvas.draw()\n",
    "#     plot_image = plot_to_image(fig)\n",
    "\n",
    "#     # Resize and overlay the plot image onto the video frame\n",
    "#     plot_image_resized = cv2.resize(plot_image, (int(frame.shape[1] * 0.4), int(frame.shape[0] * 0.8)))\n",
    "#     x_offset, y_offset = 0, 0\n",
    "#     frame[y_offset:y_offset+plot_image_resized.shape[0], x_offset:x_offset+plot_image_resized.shape[1]] = plot_image_resized\n",
    "\n",
    "#     cv2.imshow('Frame with Time Series', frame)\n",
    "#     out_video.write(frame)  # Save the frame to the output video\n",
    "\n",
    "#     # Quit on 'q' key press\n",
    "#     key = cv2.waitKey(1) & 0xFF\n",
    "#     if key == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Cleanup resources\n",
    "# cap.release()\n",
    "# out_video.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LabWork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
